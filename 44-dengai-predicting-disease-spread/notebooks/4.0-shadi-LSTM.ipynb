{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change from 3\n",
    "\n",
    "- LSTM instead of RF\n",
    "- normalizing data to [-1,+1]\n",
    "\n",
    "For reference, check https://github.com/drivendata/benchmarks/blob/master/dengue-benchmark-statsmodels.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import load_raw\n",
    "\n",
    "df_all = load_raw()\n",
    "df_all.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['features_train', 'features_test']:\n",
    "    df_all[k] = df_all[k].groupby('city').apply(lambda group: group.fillna(method='ffill'))\n",
    "    assert ~(pd.isnull(df_all[k]).any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## append without seasonality\n",
    "\n",
    "Copied from notebook 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diff = 1\n",
    "for k in ['features_train', 'features_test']:\n",
    "    temp_no = (df_all[k]\n",
    "               .groupby('city', as_index=False)\n",
    "               .apply(lambda group: group.diff(periods=n_diff).iloc[n_diff:])\n",
    "               .reset_index(level=0, drop=True)\n",
    "              )\n",
    "    temp_no.columns = [\"%s_diff\"%x for x in temp_no.columns]\n",
    "    assert ~(pd.isnull(temp_no).any().any())\n",
    "    \n",
    "    temp_yes = (df_all[k]\n",
    "               .groupby('city', as_index=False)\n",
    "               .apply(lambda group: group.iloc[n_diff:])\n",
    "               .reset_index(level=0, drop=True)\n",
    "              )\n",
    "    \n",
    "    df_all[k] = pd.concat([temp_yes, temp_no], axis=1)\n",
    "    print(df_all[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['features_train', 'features_test']:\n",
    "    assert ~(pd.isnull(df_all[k]).any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop n_diff points from labels as well\n",
    "for k in ['labels_train']:\n",
    "    temp_yes = (df_all[k]\n",
    "               .groupby('city', as_index=False)\n",
    "               .apply(lambda group: group.iloc[n_diff:])\n",
    "               .reset_index(level=0, drop=True)\n",
    "              )\n",
    "    \n",
    "    df_all[k] = temp_yes\n",
    "    print(df_all[k].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selected from\n",
    "# https://github.com/drivendata/benchmarks/blob/master/dengue-benchmark-statsmodels.ipynb\n",
    "#selected_features = ['reanalysis_specific_humidity_g_per_kg', \n",
    "#                 'reanalysis_dew_point_temp_k', \n",
    "#                 'station_avg_temp_c', \n",
    "#                 'station_min_temp_c']\n",
    "\n",
    "# all features\n",
    "# selected_features = df_all['features_train'].columns\n",
    "\n",
    "# from RF feature importances\n",
    "# selected_features = ['station_max_temp_c', 'reanalysis_dew_point_temp_k',\n",
    "#        'reanalysis_specific_humidity_g_per_kg', 'year', 'weekofyear',\n",
    "#        'ndvi_sw', 'ndvi_se']\n",
    "\n",
    "# from RF with diff\n",
    "selected_features = ['reanalysis_avg_temp_k_diff', 'station_avg_temp_c', 'ndvi_se_diff',\n",
    "       'station_max_temp_c', 'reanalysis_dew_point_temp_k',\n",
    "       'reanalysis_specific_humidity_g_per_kg', 'year', 'weekofyear',\n",
    "       'ndvi_sw', 'ndvi_se']\n",
    "\n",
    "assert len(set(selected_features) - set(df_all['features_train'].columns))==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['features_train'].shape, df_all['labels_train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note avoiding class bias\n",
    "x_train = (df_all['features_train']\n",
    "          .groupby(level='city', as_index=False)\n",
    "          .apply(lambda group: group.head(n=group.shape[0]*3//4))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [selected_features]\n",
    "          )\n",
    "x_test = (df_all['features_train']\n",
    "          .groupby(level='city', as_index=False)\n",
    "          .apply(lambda group: group.tail(n=group.shape[0]*1//4))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [selected_features]\n",
    "         )\n",
    "y_train = (df_all['labels_train']\n",
    "          .groupby('city', as_index=False)\n",
    "          .apply(lambda group: group.head(n=group.shape[0]*3//4))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [['total_cases']]\n",
    "         )\n",
    "y_test = (df_all['labels_train']\n",
    "          .groupby('city', as_index=False)\n",
    "          .apply(lambda group: group.tail(n=group.shape[0]*1//4))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [['total_cases']]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_train.reset_index()['city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize data to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def my_scale(df1):\n",
    "    scaler = MinMaxScaler()\n",
    "    df2 = scaler.fit_transform(df1)\n",
    "    df2 = pd.DataFrame(df2, columns=df1.columns, index=df1.index)\n",
    "    return df2, scaler\n",
    "\n",
    "xtrain_scaled, scaler_xtrain = my_scale(x_train)\n",
    "xtest_scaled, scaler_xtest = my_scale(x_test)\n",
    "ytrain_scaled, scaler_ytrain = my_scale(y_train)\n",
    "ytest_scaled, scaler_ytest = my_scale(y_test)\n",
    "\n",
    "xtrain_scaled.shape, xtest_scaled.shape, ytrain_scaled.shape, ytest_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lahead = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create rolling windows for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_group(group):\n",
    "    out = []\n",
    "    for i in range(lahead):\n",
    "        out.append(group.shift(i).values)\n",
    "        \n",
    "    out = np.stack(out, axis=2)[5:, :, :] # drop first lahead\n",
    "    out = np.swapaxes(out, 1, 2)\n",
    "    return out\n",
    "\n",
    "    \n",
    "xtrain_roll = xtrain_scaled.groupby(level='city').apply(stride_group)\n",
    "xtest_roll  = xtest_scaled.groupby(level='city').apply(stride_group)\n",
    "xtrain_roll.shape, xtest_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xtrain_roll.loc['sj'].shape, xtest_roll.loc['sj'].shape, xtrain_roll.loc['iq'].shape, xtest_roll.loc['iq'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_roll.loc['sj'][:3,:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_roll = ytrain_scaled.groupby(level='city', as_index=False).apply(lambda group: group.iloc[5:]).reset_index(level=0, drop=True)\n",
    "ytest_roll = ytest_scaled.groupby(level='city', as_index=False).apply(lambda group: group.iloc[5:]).reset_index(level=0, drop=True)\n",
    "ytrain_roll.shape, ytest_roll.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP drop 1st x rows if they are not a multiple of batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_roll.loc['sj'].shape, batch_size, xtrain_roll.loc['sj'].shape[0]%(32*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    xtrain_roll.loc[city] = xtrain_roll.loc[city][(xtrain_roll.loc[city].shape[0]%batch_size):]\n",
    "    ytrain_roll.loc[city] = ytrain_roll.loc[city][(ytrain_roll.loc[city].shape[0]%batch_size):]\n",
    "    xtest_roll.loc[city] = xtest_roll.loc[city][(xtest_roll.loc[city].shape[0]%batch_size):]\n",
    "    ytest_roll.loc[city] = ytest_roll.loc[city][(ytest_roll.loc[city].shape[0]%batch_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[(xtrain_roll.loc[city].shape, xtest_roll.loc[city].shape) for city in ['sj','iq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(ytrain_roll.loc[city].shape, ytest_roll.loc[city].shape) for city in ['sj','iq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "696%32, ytrain_roll.loc['sj'].shape[0], batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100,\n",
    "              input_shape=(lahead, len(selected_features)),\n",
    "              batch_size=batch_size))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = {}\n",
    "for city in ['sj', 'iq']:\n",
    "    print(city)\n",
    "    mod1[city] = create_model()\n",
    "    print(mod1[city].summary())\n",
    "    mod1[city].fit(xtrain_roll.loc[city],\n",
    "             ytrain_roll.loc[city],\n",
    "             batch_size=batch_size,\n",
    "             epochs=10,\n",
    "             verbose=1,\n",
    "             validation_data=(xtest_roll.loc[city], ytest_roll.loc[city]),\n",
    "             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to int since we know the label is integer\n",
    "predictions = (ytest_roll.copy()*0).astype('int')\n",
    "\n",
    "predictions.loc['sj'] = mod1['sj'].predict(xtest_roll.loc['sj'], batch_size=batch_size)#.astype(int)\n",
    "predictions.loc['iq'] = mod1['iq'].predict(xtest_roll.loc['iq'], batch_size=batch_size)#.astype(int)\n",
    "\n",
    "# FIXME cannot really apply scaler_ytest on the predictions\n",
    "predictions.loc[:] = scaler_ytest.inverse_transform(predictions).astype(int)\n",
    "\n",
    "predictions.loc['sj'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(city, mod1[city].evaluate(xtest_roll.loc[city], ytest_roll.loc[city], batch_size=batch_size)) for city in ['sj','iq']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj', 'iq']:\n",
    "    plt.plot(y_test.loc[city], label='actual')\n",
    "    plt.plot(predictions.loc[city], label='predicted')\n",
    "    plt.title(city)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set in submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['submission'].loc['sj'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to int since we know the label is integer\n",
    "predictions = (df_all['submission'][['total_cases']]\n",
    "               .groupby(level='city', as_index=False)\n",
    "               .apply(lambda group: group.iloc[(lahead+n_diff+1):])\n",
    "               .reset_index(level=0, drop=True)\n",
    "               .copy()\n",
    "               *0\n",
    "              ).astype('int')\n",
    "\n",
    "x_submit = (df_all['features_test']\n",
    "          .groupby(level='city', as_index=False)\n",
    "          .apply(lambda group: group.iloc[n_diff:])\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [selected_features]\n",
    "          )\n",
    "xsubmit_scaled, scaler_xsubmit = my_scale(x_submit)\n",
    "xsubmit_roll = xsubmit_scaled.groupby(level='city').apply(stride_group)\n",
    "\n",
    "print(\n",
    "    predictions.loc['sj', 'total_cases'].shape,\n",
    "    mod1['sj'].predict(xsubmit_roll.loc['sj'], batch_size=batch_size).shape\n",
    ")\n",
    "\n",
    "predictions.loc['sj', 'total_cases'] = mod1['sj'].predict(xsubmit_roll.loc['sj'], batch_size=batch_size)\n",
    "predictions.loc['iq', 'total_cases'] = mod1['iq'].predict(xsubmit_roll.loc['iq'], batch_size=batch_size)\n",
    "\n",
    "# FIXME cannot really apply scaler_ytest on the predictions\n",
    "predictions.loc[:] = scaler_ytest.inverse_transform(predictions).astype(int)\n",
    "\n",
    "print(predictions.shape)\n",
    "\n",
    "predictions.loc['sj'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.groupby(level='city').head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = df_all['submission'].copy()\n",
    "# TODO if this matches indeces properly, review the complicated merge in 3.1\n",
    "submit['total_cases'] = predictions\n",
    "submit = submit.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.groupby(level='city').head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    plt.plot(submit.loc[city, 'total_cases'].values, label=city)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to result of notebook 1-...ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn_prev = '1-submission_20180530_092740-score_29.csv'\n",
    "fn_prev = '3.0-submission_20180530_141052.csv'\n",
    "df_prev = (pd.read_csv('data/interim/%s'%fn_prev)\n",
    "             .merge(submit.reset_index(), how='left', on=['city', 'year', 'weekofyear'], suffixes=['_prev', '_curr'])\n",
    "          ).set_index(['city', 'week_start_date'])\n",
    "df_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj', 'iq']:\n",
    "    plt.plot(df_prev.loc[city, 'total_cases_prev'].values, label='prev')\n",
    "    plt.plot(df_prev.loc[city, 'total_cases_curr'].values, label='curr')\n",
    "    plt.title(city)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import make_submission"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "make_submission(submit.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
