{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/44544766/ddg#44547144\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_read_csv(fn:str):\n",
    "    return pd.read_csv(fn, na_values=['NO_LABEL', '(blank)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peek at train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd_read_csv('data_in/TrainingData.csv')\n",
    "assert not train['Unnamed: 0'].duplicated().any()\n",
    "train = train.set_index(\"Unnamed: 0\")\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peek at test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd_read_csv('data_in/TestData.csv')\n",
    "assert not test['Unnamed: 0'].duplicated().any()\n",
    "test = test.set_index(\"Unnamed: 0\")\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.loc[134338]\n",
    "# pd.isnull(train['Function']).any()\n",
    "# pd.isnull(test['Function']).any()\n",
    "set(train.columns) - set(test.columns), set(test.columns) - set(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function = Teacher Compensation   =>  Use = Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train['Use'][train['Function']=='Teacher Compensation'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position Extra = TEACHER   =?=>  Function = Teacher Compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Function'][train['Position_Extra']=='TEACHER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Position_Extra'][train['Function']=='Teacher Compensation'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Function'][train['Position_Extra']=='TEACHER'].shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['Position_Extra']=='TEACHER'].shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "labels = yaml.load(open(\"labels.yml\",'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function': ['Aides Compensation\n",
    "prediction_names = []\n",
    "for k,v1 in labels.items():\n",
    "    for v2 in v1:\n",
    "        prediction_names.append(\"%s__%s\"%(k,v2))\n",
    "        \n",
    "        \n",
    "assert 'Function__Aides Compensation' in prediction_names\n",
    "prediction_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(train.columns).intersection(set(test.columns)) - set(['FTE','Total']))\n",
    "features.sort()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[features].loc[134338].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['joined'] = train[features].fillna('').apply(lambda x: \"~\".join([y.replace(' ','').replace('\"','') for y in x]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = train['joined'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.sort_values(ascending=False).head(n=15).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = stats.sort_index()\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=stats.index[10]\n",
    "stats.index.get_loc(k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(x=range(stats.shape[0]), height=stats.values)\n",
    "# n_pts = 500000\n",
    "# plt.bar(x=range(n_pts), height=stats.iloc[:n_pts])\n",
    "plt.plot(stats.sort_values(ascending=False).values.cumsum())\n",
    "plt.title(\"%s, %s\"%(stats.values.sum(), stats.shape[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = stats.sort_values(ascending=False).reset_index().loc[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['joined']==k1].shape, train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['joined']==k1]['Function'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The 1st version turned out to have indeces not in stats.index .. weird\n",
    "# prob_fn = 'data_out/t1_probabilities_function.pkl'\n",
    "\n",
    "# Fixing the above in the 2nd version\n",
    "# prob_fn = 'data_out/t1_probabilities_function_v2.pkl'\n",
    "\n",
    "# Set \".sort()\" on features so that they're replicable\n",
    "prob_fn = 'data_out/t1_probabilities_function_v3.pkl'\n",
    "\n",
    "os.path.exists(prob_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(prob_fn):\n",
    "    probabilities = pd.read_pickle(prob_fn)\n",
    "else:\n",
    "    probabilities = pd.DataFrame(\n",
    "        np.zeros(shape=(len(labels['Function']), stats.shape[0])),\n",
    "        columns=stats.index,\n",
    "        index=labels['Function']\n",
    "    )\n",
    "    #probabilities.head(n=2)\n",
    "\n",
    "    # k1 = stats.index[0]\n",
    "    # probabilities[k1].update(train[train['joined']==k1]['Function'].value_counts())\n",
    "    # probabilities[k1]\n",
    "\n",
    "    n = len(stats.index)\n",
    "    for i,k1 in enumerate(stats.index):\n",
    "        if i % 1000 == 0: print(\"%s .. %s / %s\"%(time.ctime(), i,n))\n",
    "        probabilities[k1].update(train[train['joined']==k1]['Function'].value_counts())\n",
    "\n",
    "    # save\n",
    "    probabilities.to_pickle(prob_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k1 = stats.index[1]\n",
    "# probabilities[k1], (probabilities / probabilities.sum(axis=0))[k1]\n",
    "probabilities = probabilities / probabilities.sum(axis=0)\n",
    "\n",
    "probabilities = probabilities.transpose()\n",
    "\n",
    "probabilities = probabilities.sort_index()\n",
    "\n",
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(stats.index) == set(probabilities.index)\n",
    "assert len(set(stats.index) - set(probabilities.index)) == 0\n",
    "assert len(set(probabilities.index) - set(stats.index)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring back original set of columns\n",
    "These are the fields that got joined with ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = train[~train.duplicated(['joined'])][features+['joined']].set_index('joined').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(probabilities.index) == set(vocabulary.index)\n",
    "assert len(set(probabilities.index) - set(vocabulary.index)) == 0\n",
    "assert len(set(vocabulary.index) - set(probabilities.index)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# append \"set_index\" as recommended in pandas github issue 7632\n",
    "# https://github.com/pandas-dev/pandas/issues/7632#issuecomment-316806258\n",
    "# prob2 = probabilities.merge(vocabulary, left_index=True, right_on='joined', how='left').set_index('joined')\n",
    "prob2 = probabilities.merge(vocabulary, left_index=True, right_index=True, how='left') # .set_index('joined')\n",
    "probabilities.shape, prob2.shape # , prob2.head(n=2), prob2.head(n=2).index # , train.loc[70455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2[features].sort_index().head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.ctime())\n",
    "prob3 = prob2[features].apply(lambda x: pd.factorize(x)[0], axis=0)\n",
    "prob3 = prob3 + 1 # +1 for the -1 (keras Embedding supports [0,N) )\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob3.max().max(), prob3.min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob3.shape, probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob3.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = prob3\n",
    "y = probabilities.fillna(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras embedding + Dense/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = prob3.max(axis=0) + 1 # +1 for the 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size.index, prob3.columns\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Flatten, LSTM, Input, Concatenate, Add, Lambda\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "\n",
    "# vocab_size = stats.shape[0]\n",
    "\n",
    "# inputs = [Input(shape=(prob3.shape[1],)) for f in vocab_size.index]\n",
    "inputs = {f: Input(shape=(1,), name=f) for f in vocab_size.index}\n",
    "\n",
    "# embeddings = [Embedding(vocab_size[f], embedding_dim, input_length=prob3.shape[1]) for f in vocab_size.index]\n",
    "\n",
    "if False:\n",
    "    embedding_dim = 12 # 2 # 64 # FIXME\n",
    "    embeddings = [Embedding(vocab_size[f], embedding_dim, input_length=1)(inputs[f]) for f in vocab_size.index]\n",
    "else:\n",
    "    embeddings = [Embedding(vocab_size[f], vocab_size[f]//15, input_length=1)(inputs[f]) for f in vocab_size.index]\n",
    "\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, input_length, embedding_dim), where None is the batch dimension.\n",
    "\n",
    "x1 = Concatenate()(embeddings)\n",
    "\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(1000)(x1)\n",
    "o1 = Dense(probabilities.shape[1], activation = 'sigmoid', name='pred_prob')(x1)\n",
    "o2 = Lambda(lambda x: K.sum(x, axis=1, keepdims=True), name='sum_prob')(o1)\n",
    "outputs = [o1, o2]\n",
    "\n",
    "inputs = [inputs[f] for f in vocab_size.index]\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile('rmsprop', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    # pd.get_dummies(train3['x'].values),\n",
    "    # # train2[list(set(train2.columns) - set(['joined']))],\n",
    "    # train3['y'].values,\n",
    "    [x_train[f].values for f in vocab_size.index],\n",
    "    [y_train, np.ones(shape=(y_train.shape[0],1))],\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    validation_split = 0.2,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([x_test[f].values for f in vocab_size.index], [y_test, np.ones(shape=(y_test.shape[0],1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argmax accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_score = np.zeros(y_test.shape[0], dtype='uint8')\n",
    "y_pred, sum_pred = model.predict([x_test[f].values for f in vocab_size.index])\n",
    "for i in range(y_test.shape[0]):\n",
    "    v1 = y_test.iloc[i].idxmax()\n",
    "    v2 = probabilities.columns[np.argmax(y_pred[i])]\n",
    "    my_score[i] = 1 if (v1 == v2) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(my_score), my_score.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(probabilities.shape[1]):\n",
    "    n_show = 1000\n",
    "    y_pred, sum_pred = model.predict([x_test[f].values[:n_show] for f in vocab_size.index])\n",
    "\n",
    "    plt.figure(figsize=(20,3))\n",
    "    plt.plot(y_pred[:n_show,i], label='pred')\n",
    "    plt.plot(sum_pred, label='sum_pred', alpha=0.2)\n",
    "    plt.plot(y_test.iloc[:n_show,i].values, '.', label='actual')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(y_test.columns[i])\n",
    "    \n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([-.1,1.1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.iloc[0].sum(), y_pred[0].sum() # , y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temporal comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.figure(figsize=(10,3))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.bar(x=range(y_pred.shape[1]), height=y_test.iloc[i].values)    \n",
    "    plt.title('prediction')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.bar(x=range(y_pred.shape[1]), height=y_pred[i])    \n",
    "    plt.title('actual')\n",
    "    \n",
    "    # plt.title(y_test.index[i])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=None, random_state=0, verbose=2, n_jobs=-1)\n",
    "\n",
    "\n",
    "regr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prob3.columns, regr.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = regr.predict(x_test[:3]) # .round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf.sum(axis=1), y_pred_rf[0], y_test[:3].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
